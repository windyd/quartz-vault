


## [[01-12-23]]
ä»Šå¤©ä¸»è¦åš embedding åˆ†æ

æ‰“ç®—ä½¿ç”¨çš„ code
`D:\windyd\Projects\lab-32\LAB-22\embedding_LLM\`

- 10:12 started embedding
	```log
	kevin in ğŸŒ 2GPU3090Ti in lab-32/LAB-22/embedding_LLM on î‚  develop [!?] via ğŸ v3.8.13 via ğŸ…’ LLM
	â¯ python test_langchain_retriver.py --model text2vec-large-chinese --data_path /home/kevin/Data/Hue/ymd=2023-04-13/h=00/part-00000-9bac1344-0b02-4c2c-9226-a26e5f796142-c000.gz.parquet
	...
	...
	...
	...
	2023-12-01 02:10:47,827 - langchain_retrieve - INFO - Getting or creating vec store...
	2023-12-01 02:10:47,827 - langchain_retrieve - INFO - No existing faiss store found, building faiss store...
	2023-12-01 02:10:47,827 - langchain_retrieve - INFO - documents_df has size 211145
	Batches:   0%|â–                                             | 22/6599 [00:49<3:24:08,  1.86s/it]
	```
	
	211,145 çš„æ–‡æ¡£è¦ embed 3.5 å°æ—¶

- 11:29 æƒ³åˆ°æœ‰å¿…è¦åšä¸€ä¸ªç±»ä¼¼ [zejunwang1/CSTS: ä¸­æ–‡è‡ªç„¶è¯­è¨€æ¨ç†ä¸è¯­ä¹‰ç›¸ä¼¼åº¦æ•°æ®é›† (github.com)](https://github.com/zejunwang1/CSTS#chinese-sts-b-%E6%95%B0%E6%8D%AE%E9%9B%86) çš„æ•°æ®é›†
	- [ä¸­æ–‡Sentence Embeddings text2vec-base-chinese VS OpenAIEmbedding - çŸ¥ä¹ (zhihu.com)](https://zhuanlan.zhihu.com/p/623912895)
- 13:35 åœ¨ 12:50:47 ç§’å®Œæˆï¼Œå…±è€—æ—¶ 2:39:39 æ¯”é¢„æœŸè¦å¿«
	```log
	2023-12-01 02:10:47,827 - langchain_retrieve - INFO - documents_df has size 211145
	Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6599/6599 [2:39:39<00:00,  1.45s/it]
	2023-12-01 04:50:36,171 - faiss.loader - INFO - Loading faiss with AVX2 support.
	2023-12-01 04:50:36,219 - faiss.loader - INFO - Successfully loaded faiss with AVX2 support.
	2023-12-01 04:50:47,031 - langchain_retrieve - DEBUG - --> Building FAISS store <--: start_time 02:10:47 elapsed_time 9599.204355239868 s
	```
	
	211,145 å°é‚®ä»¶ ä» 143M (full) å˜æˆäº† 825M ï¼ˆæå– subject + content + attachï¼‰
	```log
	kevin in ğŸŒ 2GPU3090Ti in LAB-22/embedding_LLM/text2vec-large-chinese_faiss_store on î‚  develop [!?] via ğŸ…’ LLM
	â¯ ll -h
	total 941M
	-rw-rw-r-- 1 kevin kevin 825M Dec  1 04:50 index.faiss
	-rw-rw-r-- 1 kevin kevin 116M Dec  1 04:50 index.pkl
	```
	
	```log
	>>> import pandas as pd
	>>> df = pd.read_parquet('/home/kevin/Data/Hue/ymd=2023-04-13/h=00/part-00000-9bac1344-0b02-4c2c-9226-a26e5f796142-c000.gz.parquet', columns=['subject', 'content', 'attach'])
	>>> df.info()
	<class 'pandas.core.frame.DataFrame'>
	RangeIndex: 211145 entries, 0 to 211144
	Data columns (total 3 columns):
	 #   Column   Non-Null Count   Dtype
	---  ------   --------------   -----
	 0   subject  211145 non-null  object
	 1   content  211145 non-null  object
	 2   attach   211145 non-null  object
	dtypes: object(3)
	memory usage: 4.8+ MB
	>>> df.to_parquet('checkout_size.parquet')
	>>> exit()
	
	kevin in ğŸŒ 2GPU3090Ti in lab-32/LAB-22/embedding_LLM on î‚  develop [!?] via ğŸ v3.8.13 via ğŸ…’ LLM
	â¯ ll -h checkout_size.parquet
	-rw-rw-r-- 1 kevin kevin 46M Dec  1 05:50 checkout_size.parquet
	```
	å¦‚æœåªçœ‹ subject + content + attach çš„è¯ï¼ŒåŸæœ¬çš„æ•°æ®åªå å†…å­˜ 4.8+MB
	pandas å†™åˆ°ç¡¬ç›˜åå  46M (ä¸çŸ¥é“ pandas çš„å¤„ç†æœ‰æ²¡æœ‰è†¨èƒ€)

- 15:40 Read code `langchain` does this internally
	```python
	# save index separately since it is not picklable
	faiss = dependable_faiss_import()
	faiss.write_index(
		self.index, str(path / "{index_name}.faiss".format(index_name=index_name))
	)
	
	# save docstore and index_to_docstore_id
	with open(path / "{index_name}.pkl".format(index_name=index_name), "wb") as f:
		pickle.dump((self.docstore, self.index_to_docstore_id), f)
	```
	`self.index_to_docstore_id` is the mapping to docstore:
	`Dict[int, str]`
	
	```log
	0: 5b1d34fc-b406-4406-ab50-0436ebb98a82
	1: 23221a7c-1f63-4633-b74b-fb53614aae11
	2: 5915bf29-ec3f-4a98-b1b8-6373e039054a
	3: e520c1f9-0aa7-4026-b033-2f7cdec9e4de
	4: da8105c1-164a-4645-b9d5-e00211ab81d9
	5: 11f00be6-ffb1-4bb3-82d5-61d33cb6591b
	6: 8d2d2804-7fea-4f80-9c67-b9c46729185a
	7: 4f931f78-0d5e-4486-a5b0-e4a7e1900fb5
	8: 3b904e22-1f8a-41f1-87f6-0674007ecfbc
	9: 41cd72a3-efb1-4f46-9377-5dd80125cd61
	10: f91a01ed-9acc-4ec4-85c2-0485cb5ca4dc
	11: fbe4dc78-e2e6-401d-840d-172d01638a3d
	```
	how do we access the doc's index using this id?
	```
	docstore.search(index_to_docstore[i])
	```
	
	I don't understand why `self.index_to_docstore` has to be such a mapping
- 16:46 `Docstore` is a component of `vecstore`
	```python
	vecstore = cls(
		embedding,
		index,
		InMemoryDocstore(),
		{},
		normalize_L2=normalize_L2,
		distance_strategy=distance_strategy,
	)
	```
	
- > I don't understand why `self.index_to_docstore` has to be such a mapping
	
	å› ä¸ºè¦æ”¯æŒå¢åˆ æŸ¥æ”¹, é¡ºåºçš„æ•°å­—ä¸æ–¹ä¾¿æ“ä½œï¼Œç”¨ uuid æ¯”è¾ƒæ–¹ä¾¿ 
- 16:59 *å¯è§ int number æ˜¯ç”¨æ¥è®¿é—®å‘é‡çš„ï¼Œuuid æ˜¯ç”¨æ¥è®¿é—® docs çš„*ã€‚è¦ access å‘é‡ï¼Œsimply use ç¬¬ i ä¸ª index å°±å¥½äº† by looking at the delete function
	```python
	def delete(self, ids, **kwargs):
		...
		## Reversed mapping: ids --> i-th item
		reversed_index = {id_: idx for idx, id_ in self.index_to_docstore_id.items()}
		index_to_delete = [reversed_index[id_] for id_ in ids]
		## delete by i-th item
		self.index.remove_ids(np.array(index_to_delete, dtype=np.int64))
		self.docstore.delete(ids)
		
		remaining_ids = [
			id_
			for i, id_ in sorted(self.index_to_docstore_id.items())
			if i not in index_to_delete
		]
		self.index_to_docstore_id = {i: id_ for i, id_ in enumerate(remaining_ids)}
	
		return True
	```


## [[04-12-23]]
MAIN work files:
- Try_Phoenix.ipynb
- faiss_try.py
- test_langchain_retriever.py
- [I] å¯ä»¥ç”¨ç¼–è¾‘è·ç¦»ä¹‹ç±»çš„å¿«é€Ÿå»æ‰é‡å¤çš„é‚®ä»¶ï¼Ÿ
 - [I] ç›´æ¥ æŠŠ paquet -> embedded parquet çš„ function ä¹‹åå†å†™åˆ° `test_langchain_retriver.py` ä¹‹ä¸­
 
- 10:48 implement langchain `faiss_store` to parquet
- 11:19 å¼„æˆ data.parquet çœ‹èµ·æ¥æ›´çœç©ºé—´
	```log
	kevin in ğŸŒ 2GPU3090Ti in lab-32/LAB-22/embedding_LLM on î‚  develop [!?] via ğŸ v3.8.13 via ğŸ…’ LLM
	â¯ ll -h text2vec-large-chinese_faiss_store/
	total 941M
	-rw-rw-r-- 1 kevin kevin 825M Dec  1 04:50 index.faiss
	-rw-rw-r-- 1 kevin kevin 116M Dec  1 04:50 index.pkl
	
	kevin in ğŸŒ 2GPU3090Ti in lab-32/LAB-22/embedding_LLM on î‚  develop [!?] via ğŸ v3.8.13 via ğŸ…’ LLM
	â¯ ll -h data.parquet
	-rw-rw-r-- 1 kevin kevin 880M Dec  4 03:10 data.parquet
	```
- 11:22 å‡†å¤‡æµ‹è¯• phoenix
- 12:28 æš‚æ—¶æ— æ³•æ˜¾ç¤ºæ–‡å­—ï¼Œæƒ³åŠæ³• get it to work
- 13:39 done above, just need to pass `raw_data_column_name` to schema
- 13:56 export in phoenix --> output a parquet of *selected data*
- 14:10 HDBSCAN: å½“é‡‡ç”¨è¾ƒå¤§çš„ `min_cluster_size` æ—¶ï¼Œä¼šæ˜æ˜¾çš„æŠŠä¸­è‹±æ–‡åˆ†å‡ºæ¥
	- [!] æ³¨æ„åˆ° attach å› ä¸ºé€šå¸¸è‹±æ–‡ã€ä¹±ä¸ƒå…«ç³Ÿçš„ä¸œè¥¿æ¯”è¾ƒå¤šï¼Œæœ‰å¯èƒ½ä¼šå½±å“ç»“æœï¼ˆä»€ä¹ˆç»“æœï¼Ÿï¼‰
- 14:21 æ•°æ®è¦é‡æ–°æ‹¿ä¸€ä¸‹, åªæ‹¿ä¸­æ–‡çš„è¯•è¯•
- 15:09 ç›´æ¥ æŠŠ paquet -> embedded parquet çš„ function ä¹‹åå†å†™åˆ° `test_langchain_retriver.py` ä¹‹ä¸­
- 15:15 æ­£åœ¨ embed `/home/kevin/Data/Hue/ymd=2023-04-13/h=16/part-00003-e094963c-fecf-4aa5-887d-2c59a5fa2628-c000.gz.parquet` --> aborted
- 15:22 2 things
	1. add correct hf calling according to github
	2. make correct preprocessing
- 16:45 concerns: é™ç»´æ¯”è¾ƒä¼¤ info
	- [?] ç›®å‰çš„ exact process æ˜¯æ€æ ·çš„ï¼Ÿ
	- SOM é™ç»´ã€autoencoder é™ç»´
- 17:14 é‡æ–°ä» langchain faiss store æå– parquet
	- 17:33 å…ˆé‡æ–° embed (å› ä¸ºä¹‹å‰çš„ filter columns ä¸åŒ…å«æƒ³è¦çš„ metadata)
		```log
		2023-12-04 09:32:22,906 - sentence_transformers.SentenceTransformer - INFO - Use pytorch device: cuda
		2023-12-04 09:32:22,906 - langchain_retrieve - INFO - Getting or creating vec store...
		2023-12-04 09:32:22,906 - langchain_retrieve - INFO - No existing faiss store found, building faiss store...
		2023-12-04 09:32:22,906 - langchain_retrieve - INFO - documents_df has size 408703
		Batches:   2%|â–‹                                           | 195/12772 [05:06<4:46:11,  1.37s/it]
		```
- 17:34 çœ‹çœ‹ [[UMAP--Uniform Manifold Approximation and Projection for Dimension Reduction.pdf]]

## [[05-12-23]]
1. embed`test_langchain_retrieve.py` -> text2vec-large-chinese_faiss_store
1. transform_parquet(`faiss_try.py`) -> data.parquet
- 09:49 connection issue? jupyter to 3090Ti seems laggy
- 10:40 æ‰¾åˆ°ä¸€ç»„ä¸é”™çš„å‚æ•°
![[LAB21 2023-12-05 10.51.03.excalidraw]]
- 12:53 æœ‰å¯èƒ½ phoenix åªèƒ½å¤„ç†å°éƒ¨åˆ†æ•°æ®
	- æ€ä¹ˆç”¨åœ¨å¤§æµé‡ä¸Šï¼Ÿ
	- æ–¹æ¡ˆä¸€ï¼šå°èŒƒå›´èšå¥½ç±»
		- option aï¼šdist to centroids æ¥æ¨æ–­
		- option bï¼šå¯†åº¦å¯è¾¾æ¨æ–­
	- æ–¹æ¡ˆäºŒï¼šç›¸åŒçš„å‚æ•°æ”¾åˆ° spark è·‘
- 12:59 faiss -> k-means res 
- 15:07 [Benchmarking Performance and Scaling of Python Clustering Algorithms â€” hdbscan 0.8.1 documentation](https://hdbscan.readthedocs.io/en/latest/performance_and_scalability.html) some proof that k-means seems garbage
- 15:26 Decided to test HDBSCAN myself
- 15:38 è‡ªå·±ç›´æ¥è¿›è¡Œ hdbscan å¾—ä¸åˆ°å¥½çš„ç»“æœï¼ˆ7/10ï¼‰éƒ½æ˜¯å¼‚å¸¸ç‚¹ï¼Œå‚è€ƒä¸€ä¸‹ Phoenix
	```python
	vectors, clustered_events = PointCloud(
		dimensionalityReducer=Umap(n_neighbors=n_neighbors, min_dist=min_dist),
		clustersFinder=Hdbscan(
			min_cluster_size=min_cluster_size,
			min_samples=cluster_min_samples,
			cluster_selection_epsilon=cluster_selection_epsilon,
		),
	).generate(data, n_components=n_components)
	```
- 15:41 ä¸€ç›®äº†ç„¶äº†ï¼Œç¡®å®æ˜¯å…ˆé™äº†ç»´
	```python
	@dataclass(frozen=True)
	class PointCloud:
	    dimensionalityReducer: DimensionalityReducer
	    clustersFinder: ClustersFinder
	
	    def generate(
	        self,
	        data: Mapping[ID, Vector],
	        n_components: int = 3,
	    ) -> Tuple[Dict[ID, Vector], Dict[str, Set[ID]]]:
	        """
	        Given a set of vectors, projects them onto lower dimensions, and
	        finds clusters among the projections.
	
	        Parameters
	        ----------
	        data : mapping
	            Mapping of input vectors by their EventIds.
	
	        n_components: int, default=3
	            Number of dimensions in the projected space.
	
	        Returns
	        -------
	        projections : dictionary
	            Projected vectors in the low dimensional space, mapped back to the
	            input vectors' EventIds.
	
	        cluster_membership: dictionary
	            Cluster membership by way of cluster_ids in the form of integers
	            0,1,2,... mapped back to the input vectors' EventIds. Note that
	            some vectors may not belong to any cluster and are excluded here.
	
	        """
	
	        if not data:
	            return {}, {}
	        event_ids, vectors = zip(*data.items())
	        projections = self.dimensionalityReducer.project(
	            np.stack(vectors), n_components=n_components
	        )
	        clusters = self.clustersFinder.find_clusters(projections)
	        return dict(zip(event_ids, projections)), {
	            str(i): {event_ids[row_index] for row_index in cluster}
	            for i, cluster in enumerate(clusters)
	        }
	```
- 15:56 projector(`Umap`) is a wrapper of `umap.UMAP` https://github.com/Arize-ai/phoenix/blob/1562309da191524761194c2d73299e048194ad25/src/phoenix/pointcloud/projectors.py#L23C9-L23C9
	```python
	import warnings
	from dataclasses import asdict, dataclass
	from typing import cast
	
	import numpy as np
	import numpy.typing as npt
	from typing_extensions import TypeAlias
	
	with warnings.catch_warnings():
	    from numba.core.errors import NumbaWarning
	
	    warnings.simplefilter("ignore", category=NumbaWarning)
	    from umap import UMAP
	
	Matrix: TypeAlias = npt.NDArray[np.float64]
	
	
	def _center(arr: Matrix) -> Matrix:
	    return cast(Matrix, arr - np.mean(arr, axis=0))
	
	
	@dataclass(frozen=True)
	class Umap:
	    n_neighbors: int = 15
	    min_dist: float = 0.1
	
	    def project(self, mat: Matrix, n_components: int) -> Matrix:
	        config = asdict(self)
	        config["n_components"] = n_components
	        if len(mat) <= n_components:
	            # init='spectral', the default, cannot be used when n_components
	            # is greater or equal to the number of samples.
	            # see https://github.com/lmcinnes/umap/issues/201#issuecomment-462097103
	            config["init"] = "random"
	        return _center(UMAP(**config).fit_transform(mat))
	```
- 17:15 æ‰æ³¨æ„åˆ°è¿™æ¬¡çš„æ•°æ®å±…ç„¶æœ‰ 40wan
- 17:51 å¾…å‚è€ƒ --> [How To Tune HDBSCAN | by Charles Frenzel | Towards Data Science](https://towardsdatascience.com/tuning-with-hdbscan-149865ac2970)
	- > "**Kmeans** assume that data is numerical and sphere-shaped. Those types of assumptions do not fair well when the data has high dimensionality and includes categorical values."
	- Silhouette Score **does not** work for density-based algos 
		1. not considering noise in the index calculation
		2. makde use of distances
	- DBCV(Density Based Clustering Validation) [[DBCV]]
	 $$
	\text{DBCV}(C)=\sum_{i=1}^{i=l} \frac{|C_{i}|}{|O|}V_{C}(C_{i})
	$$
	- Clustering Solution: $C=\{C_{i}\}, i \le i \le l$ 
	- **Intuition**: weighted average of the Validity Index of all clusters in $C$
- Further Reading: [[What are the true clusters--2015arxiv.pdf]]
## [[06-12-23]]
- 10:59 [[DBCV]]
- 10:59 continue reading [How To Tune HDBSCAN | by Charles Frenzel | Towards Data Science](https://towardsdatascience.com/tuning-with-hdbscan-149865ac2970)
- 11:34 figured out how to use DBCV (implemented by `hdbscan` lib)
- 12:23 Come up with a task --> å†™ä¸€ä¸ª å¾ªç¯æ¥ å¯¹æ¯”ä¸åŒå‚æ•°çš„
	- è€—æ—¶
	- DBCV
		- weighted average
		- per cluster distribution -> make bins
	- anomaly ratio
	- tree plot
- 13:14 hdbscan papers
	- [[hdbscan.pdf]]
	- [[Accelerated Hierarchical Density Based Clustering.pdf]]
- 13:22 [[HDBSCAN]]
- 14:56 done reading [[HDBSCAN]] for now
- 14:56 checkout the $\lambda$ of our data
	- æœ‰çš„ååˆ†çš„å¤§,400 å¤š, æ‰“ç®— rescale data çœ‹çœ‹æ˜¯å¦ä¼šå½±å“
- 16:00 cluster_utils (plots, metric cals)
- 16:44 å‡†å¤‡ wrap `project_and_cluster` into ä¸€ä¸ªæ–¹ä¾¿è°ƒç”¨ sklearn
- 17:56 æ”¹ `sklearn.model_selection.StratifiedKFold` è®©å®ƒè¿”å› K æ¬¡ç›¸åŒåŸå°ä¸åŠ¨çš„æ•°æ®é›†
- 18:00 run grid
	```python
	param_grid = {
	    'min_samples': [1, 2, 4, 8, 16],
	    'min_cluster_size': [10, 20, 40, 80],
	    'cluster_selection_method' : ['eom'],
	}
	```
- 19:20 æœ‰ç‚¹é—®é¢˜ ç”¨äº† projection æ¥ grid search
	- ä¸ç¡®å®š Umap åœ¨input çš„ç»´åº¦ä¸º 3 æ—¶ï¼Œè®¾ç½® `n_components=3` çš„è¡Œä¸ºï¼Œ nonetheless, æœ‰ä¸€ç»„å‚æ•°ä¸Šäº† 0.4, ( as an empirical referrence 0.48 ä¸ºä¸é”™çš„æ•°å­—)
		```log
		DBCV score :0.4377849103136721
		Best Parameters {'cluster_selection_method': 'eom', 'min_cluster_size': 10, 'min_samples': 1}
		```
## [[07-12-23]]
- 09:15 Paramsearch åº”å¦‚ä¸‹
	- è€—æ—¶(ç²—ç­›æŒ‡æ ‡--> ä½œä¸ºå‚è€ƒ)
	- DBCV
		- weighted average (ç²—ç­›æŒ‡æ ‡-->æŒ‰æ­¤é€‰best)
	- anomaly ratio (ç²—ç­›æŒ‡æ ‡ --> ä½œä¸ºå‚è€ƒ)
- 09:18 ç²¾ç­›åº”å¦‚ä¸‹
	- è€—æ—¶
	- DBCV
		- weighted average
		- per cluster distribution -> make bins
	- anomaly ratio
	- tree plots
	- ç°‡å¯†åº¦ distribution
- 09:56 improve estimator class
	- [ ] predict çš„æ—¶å€™å¸¦ä¸Šè®­ç»ƒçš„ä¸€èµ·ç®— score
- 11:31 çœ‹ã€è®¨è®ºäº†ä¸€å † retvec å’Œ æ¨¡å‹è’¸é¦çš„ä¸œè¥¿
	- [BERTè’¸é¦å®Œå…¨æŒ‡å—ï½œåŸç†/æŠ€å·§/ä»£ç  (qq.com)](https://mp.weixin.qq.com/s/tKfHq49heakvjM0EVQPgHw)
- 13:40 code read [[Sklearn Model Selection]]
	- 15:57 äº†è§£åˆ°è°ƒç”¨æ€ä¹ˆèµ°åˆ° score çš„ [[Sklearn Model Selection#æœ€ç»ˆçš„ score å‡½æ•°]]
- 15:57 what's `min_sample` in HDBSCAN?
- 16:05 éœ€è¦çœ‹çœ‹ sklearn åŸç”Ÿ predictor çš„ score æ˜¯æ€ä¹ˆå®ç°çš„
	- 16:07 [sklearn.cluster.HDBSCAN â€” scikit-learn 1.3.2 documentation](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.HDBSCAN.html#sklearn.cluster.HDBSCAN) ç›´æ¥æŠ„ä¸€æŠ„è¿™é‡Œçš„ score
	- 16:10 è¿™ä¸ª class æ²¡æœ‰ score
	- 16:16 Kmeans çš„ score
		```python
		def score(self, X, y=None, sample_weight=None):
			"""Opposite of the value of X on the K-means objective.
		
			Parameters
			----------
			X : {array-like, sparse matrix} of shape (n_samples, n_features)
				New data.
		
			y : Ignored
				Not used, present here for API consistency by convention.
		
			sample_weight : array-like of shape (n_samples,), default=None
				The weights for each observation in X. If None, all observations
				are assigned equal weight.
		
			Returns
			-------
			score : float
				Opposite of the value of X on the K-means objective.
			"""
			check_is_fitted(self)
		
			X = self._check_test_data(X)
			sample_weight = _check_sample_weight(sample_weight, X, dtype=X.dtype)
		
			_, scores = _labels_inertia_threadpool_limit(
				X, sample_weight, self.cluster_centers_, self._n_threads
			)
			return -scores
		```
- 17:26 ä¸¤ä¸ªä¸œè¥¿æ€ä¹ˆä¸ä¸€æ ·ï¼Ÿ
	- ![[Pasted image 20231207172700.png]]
	- see [API Reference â€” hdbscan 0.8.1 documentation](https://hdbscan.readthedocs.io/en/latest/api.html) or below

> [`relative_validity_`](https://hdbscan.readthedocs.io/en/latest/api.html#id92): float
> 
> A fast approximation of the Density Based Cluster Validity (DBCV) score [4]. The only differece, and the speed, comes from the fact that thisÂ [relative_validity_](https://hdbscan.readthedocs.io/en/latest/api.html#id94)Â is computed using the mutual- reachability minimum spanning tree, i.e.Â [minimum_spanning_tree_](https://hdbscan.readthedocs.io/en/latest/api.html#id96), instead of the all-points minimum spanning tree used in the reference. This score might not be an objective measure of the goodness of clusterering. It may only be used to compare results across different choices of hyper-parameters, therefore is only a relative score.

## [[08-12-23]]
- [I] æ€è·¯ï¼š3d ä¸Šè°ƒå¥½ umap å‚æ•°ï¼Œåœ¨è¿›è¡Œèšç±»

**é—®é¢˜åˆ†æ**ï¼š
- ä¸¤å¤§éš¾é¢˜ï¼š
	- é«˜ç»´åº¦ç©ºé—´çš„é—®é¢˜ (regardless of ç®—æ³•)
	- è¯­ä¹‰å±€éƒ¨ç»“æ„é—®é¢˜ï¼šè¯­ä¹‰å¯èƒ½ä¸ä»…æ˜¯ä¸ªçƒ, åŒæ ·ç±»åˆ«çš„è¯­ä¹‰å¯èƒ½åœ¨ä¸€ä¸ªè¶…å¹³é¢ä¸Š 

- è§£å†³é«˜ç»´åº¦é—®é¢˜ -> é™ç»´
	- PCA -> ç”¨çº¿æ€§ï¼ˆæ—‹è½¬ã€ä¼¸ç¼©ã€æŠ•å½±ï¼‰çš„æ–¹æ³•é‡æ–°æ‰¾åˆ°ä¸€ç»„åæ ‡è½´ï¼ŒæŠŠä¿¡æ¯é‡ï¼ˆæ–¹å·®ï¼‰ä½çš„åæ ‡è½´å»æ‰
	- [[My Work/Concepts/t-SNE]]
		- å»ºç«‹ é«˜ç»´æ•°æ®åˆ†å¸ƒï¼ˆGassuainï¼‰ã€ä½ç»´æ•°æ®åˆ†å¸ƒ (learnable)
		- ç”¨ KL æ•£åº¦æ‹‰è¿‘ä¸¤ä¸ªåˆ†å¸ƒçš„è·ç¦»
		- [i] the Gaussian teacher maybe not good enough
	- [[My Work/Concepts/UMAP]]
		- å»ºç«‹ é«˜ç»´æ•°æ®åˆ†å¸ƒï¼ˆæŸç§æ›²é¢ä¸Šçš„åˆ†å¸ƒï¼‰ã€ä½ç»´æ•°æ®åˆ†å¸ƒ (learnable)
		- ç”¨ CE æ‹‰è¿‘ä¸¤ä¸ªåˆ†å¸ƒçš„è·ç¦»
		- [?] Umap èƒ½å¤Ÿå¤šå¤§ç¨‹åº¦çš„å­¦åˆ°å±€éƒ¨ç»“æ„ï¼Ÿ
	- Autoencoder
		- mlp é™ç»´ mlp å‡ç»´
		- minimize é‡æ„è¯¯å·®
		- [!] é«˜èƒ½è€—
		- [!] æ³›åŒ–æ€§éš¾ä»¥ä¿è¯ ï¼ˆä¸æ˜¯ç›´æ¥åœ¨æ¦‚ç‡ç©ºé—´ä¸Šå»ºæ¨¡ï¼‰--> å¯ç”¨æ€§å˜çª„ï¼Œä¸ºäº†é™ç»´è¿™ä¸ªä»»åŠ¡è¦å¤šæ¬¡è°ƒæ•´è¿™ä¸ªæ¨¡å‹

- è§£å†³å±€éƒ¨ç»“æ„é—®é¢˜
	- è¿é€šå›¾èƒ½è¢« HDBSCAN capture åˆ°

**å½“å‰å‘ç°**ï¼š
- é™ç»´ååˆ†å¿…è¦ (ä»è´¨é‡çš„è§’åº¦)
- HDBSCAN çš„è°ƒå‚ååˆ†å®¹æ˜“ ï¼ˆç›¸å¯¹äº UMAPï¼‰
- èšç±»çš„æ•ˆæœ largely depends on UMAP

**ç­–ç•¥**ï¼š
ç”Ÿæˆè¿‡ç¨‹
- äººå·¥ç²¾è°ƒ umap
- æœºå™¨æš´åŠ›æœç´¢ HDBSCAN è¶…å‚ -> è¾“å‡ºèšç±»

è¯„ä¼°è¿‡ç¨‹
- å¯¹è¾“å‡ºçš„èšç±»è¿›è¡Œæ€»ç»“ï¼ˆLLM or ä¸»é¢˜æ¨¡å‹ï¼Ÿ )
- è¾“å‡ºèšç±»/ä¸»é¢˜æ•°é‡

LOG
- 14:57 è°ƒå‚ UMAP on 10,000 sample points
	- æ‹›è˜ä¿¡æ¯ï¼š
		![[Pasted image 20231208145934--quartz.png|214]]



## UMAP and HDBSCAN Tuning
> started:: [[05-12-23]]


ç”±äºè‡ªç”±åº¦æ²¡é‚£ä¹ˆé«˜ï¼Œæš‚æ—¶åªç”¨ umap + HDBSCAN æ¥åš instead of the wrapped phoenix

`min_distance`: `[0,1)`

- [!] æ³¨æ„ï¼šå½“è°ƒæ•´ umap çš„æ—¶å€™ï¼Œhdbscan çš„æ•°é‡ä¹Ÿä¼šå˜
- [x] çœ‹çœ‹ phoenix çš„å®ç° 
	- ç¡®å®æ˜¯å…ˆé™åˆ° 3 dimï¼Œå†èšç±»

top3 bottom3 mid`n` 

### Base param set

> [!NOTE] `n_sample` 10000 ä¸‹çš„è¿˜è¡Œçš„å‚æ•°, å°±æ˜¯æœ‰ç‚¹æŒ¤(UMAP)
> clusters: "D:\windyd\Projects\lab-32\LAB-22\experiments\tune-HDBSCAN\base_param"
> ![[Pasted image 20231205112241.png|475]]
> INPUT
> - HDBSCAN
> 	- `min_cluster_size`: 40
> 	- `cluster_min_sample`: 10
> 	- `cluster_selection_epsilon`: 0
> - UMAP
> 	- `min_distance`: 0
> 	- `n_neighbors`: 100 (max already)
> 	- `n_sample`: 10000
> 
> OUTPUT
> - num_cluster: 276 (å¯èƒ½ä¼šå˜)
> - normal points: 7160


#### Tune Min Size

##### min_cluster_size 80
> [!NOTE] min_cluster_size 80
> ![[Pasted image 20231205135751.png|475]]
> INPUT
> - HDBSCAN
> 	- `min_cluster_size`: 80
> 	- `cluster_min_sample`: 10
> 	- `cluster_selection_epsilon`: 0
> - UMAP
> 	- `min_distance`: 0
> 	- `n_neighbors`: 100 (max already)
> 	- `n_sample`: 10000
> 
> OUTPUT
> - num_cluster: 5 (å¯èƒ½ä¼šå˜)
> - normal points: 9660





### Deprecated due to random seed
#### Base param set(deprecated)

> [!NOTE] `n_sample` 10000 ä¸‹çš„è¿˜è¡Œçš„å‚æ•°, å°±æ˜¯æœ‰ç‚¹æŒ¤(UMAP)
> clusters: "D:\windyd\Projects\lab-32\LAB-22\experiments\tune-HDBSCAN\base_param"
> ![[Pasted image 20231205112241.png|475]]
> INPUT
> - HDBSCAN
> 	- `min_cluster_size`: 40
> 	- `cluster_min_sample`: 10
> 	- `cluster_selection_epsilon`: 0
> - UMAP
> 	- `min_distance`: 0
> 	- `n_neighbors`: 100 (max already)
> 	- `n_sample`: 10000
> 
> OUTPUT
> - num_cluster: 40 (å¯èƒ½ä¼šå˜)
> - normal points: 6442


#### Tune Min Size

##### min_cluster_size 80(deprecated)
> [!NOTE] min_cluster_size 80
> ![[Pasted image 20231205135751.png|475]]
> INPUT
> - HDBSCAN
> 	- `min_cluster_size`: 80
> 	- `cluster_min_sample`: 10
> 	- `cluster_selection_epsilon`: 0
> - UMAP
> 	- `min_distance`: 0
> 	- `n_neighbors`: 100 (max already)
> 	- `n_sample`: 10000
> 
> OUTPUT
> - num_cluster: 21 (å¯èƒ½ä¼šå˜)
> - normal points: 5935



